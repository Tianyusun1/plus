# scripts/train.py (V10.0: Enhanced Text-to-Gestalt Learning)

# --- å¼ºåˆ¶æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python æ¨¡å—æœç´¢è·¯å¾„ ---
import sys
import os
import argparse 

# è·å–å½“å‰è„šæœ¬ (train.py) çš„ç»å¯¹è·¯å¾„
current_script_path = os.path.abspath(__file__)
# è·å–é¡¹ç›®æ ¹ç›®å½• (train.py çš„çˆ¶ç›®å½•)
project_root = os.path.dirname(os.path.dirname(current_script_path))
# å°†é¡¹ç›®æ ¹ç›®å½•æ’å…¥åˆ° sys.path çš„å¼€å¤´
sys.path.insert(0, project_root)

# --- ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥é¡¹ç›®å†…éƒ¨æ¨¡å— ---
import torch
from torch.utils.data import DataLoader
import yaml
from transformers import BertTokenizer
from data.dataset import PoegraphLayoutDataset, layout_collate_fn 
from models.poem2layout import Poem2LayoutGenerator
import trainers 
from trainers.rl_trainer import RLTrainer 
from trainers.loss import analyze_semantic_prior_coverage, get_gestalt_loss_weights  # [NEW V10.0]
from collections import Counter
import numpy as np 

# --- è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—ç±»åˆ«æƒé‡ (è§£å†³ç±»åˆ«åå·®é—®é¢˜) ---
def compute_class_weights(dataset, num_classes: int, max_weight_ratio: float = 3.0):
    """
    è®¡ç®—æ•°æ®é›†å†…æ‰€æœ‰å¸ƒå±€å…ƒç´ çš„ç±»åˆ«é¢‘ç‡ï¼Œå¹¶è¿”å›åå‘é¢‘ç‡æƒé‡ã€‚
    è¿”å› (num_classes + 1) ä¸ªæƒé‡ï¼Œå…¶ä¸­ç´¢å¼• 0 å¯¹åº” EOSã€‚
    """
    element_class_counts = Counter()
    
    # éå†æ•´ä¸ªæ•°æ®é›†è®¡ç®—æ‰€æœ‰å…ƒç´ å®ä¾‹çš„ç±»åˆ«è®¡æ•°
    for sample in dataset.data:
        # boxes æ˜¯ List[(cls_id, cx, cy, w, h)]ï¼Œcls_id æ˜¯ 2.0 - 10.0
        for cls_id_float, _, _, _, _ in sample['boxes']:
            # æ˜ å°„åˆ°å†…éƒ¨ ID 0-8 (å¯¹åº”å…ƒç´  2-10)
            internal_element_id = int(cls_id_float) - 2
            if 0 <= internal_element_id < num_classes:
                element_class_counts[internal_element_id] += 1
                
    total_count = sum(element_class_counts.values())
    
    # æœ€ç»ˆæƒé‡æ•°ç»„å¤§å°å¿…é¡»æ˜¯ 9 (å…ƒç´ ) + 1 (EOS) = 10
    final_num_classes = num_classes + 1 
    
    if total_count == 0:
        print("[Warning] No valid elements found in dataset for weight calculation. Using uniform weights.")
        return torch.ones(final_num_classes)

    # åˆå§‹åŒ–æƒé‡: size 10 (ç´¢å¼• 0 for EOS, ç´¢å¼• 1-9 for elements)
    weights = torch.zeros(final_num_classes) 
    
    # è®¡ç®— 9 ä¸ªå…ƒç´  (å†…éƒ¨ ID 0-8) çš„æƒé‡ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨ç´¢å¼• 1-9
    for i in range(num_classes): # i goes from 0 to 8 (internal element ID)
        frequency = element_class_counts.get(i, 0) / total_count
        
        # å°†å…ƒç´ çš„å†…éƒ¨ ID i (0-8) æ˜ å°„åˆ°æ–°çš„ç´¢å¼• i+1 (1-9)
        new_index = i + 1 
        
        if frequency > 0:
            # é‡‡ç”¨ log(1.0 + x) æˆ– log(1.02 + x) æ¥å¹³æ»‘å’Œåè½¬é¢‘ç‡
            weights[new_index] = 1.0 / np.log(1.02 + frequency) 
        else:
            # å¯¹äºç¨€æœ‰/ä¸å­˜åœ¨çš„ç±»åˆ«ï¼Œèµ‹äºˆæœ€é«˜çš„æƒé‡
            weights[new_index] = 1.0 / np.log(1.02 + 1e-6) # èµ‹äºˆæœ€å¤§æƒé‡
            
    # ä¸º EOS ç±» (ç´¢å¼• 0) åˆ†é…æƒé‡
    # æˆ‘ä»¬ä½¿ç”¨è®¡ç®—å‡ºçš„å…ƒç´ æƒé‡çš„å¹³å‡å€¼ä½œä¸ºåŸºçº¿
    avg_element_weight = weights[1:].sum() / num_classes if num_classes > 0 else 1.0
    weights[0] = avg_element_weight # å°†å¹³å‡æƒé‡åˆ†é…ç»™ EOS (ç´¢å¼• 0)
            
    # æƒé‡é’³ä½å’Œæ ‡å‡†åŒ–
    # å¯¹æ‰€æœ‰ 10 ä¸ªç±»åˆ«è®¡ç®—å¹³å‡æƒé‡
    avg_weight = weights.mean()
    max_allowed_weight = avg_weight * max_weight_ratio
    weights = torch.clamp(weights, max=max_allowed_weight)
    
    # é‡æ–°å½’ä¸€åŒ–ï¼Œç¡®ä¿æ€»å’Œæ˜¯ final_num_classes (10)
    weights = weights / weights.sum() * final_num_classes
    
    return weights.float()
# ------------------------------------------

# =============================================================================
# [NEW V10.0] æ•°æ®é›†ç»Ÿè®¡åˆ†æå‡½æ•°
# =============================================================================
def analyze_dataset_statistics(dataset):
    """
    åˆ†ææ•°æ®é›†çš„æ€åŠ¿æå–è´¨é‡å’Œè¯­ä¹‰å…ˆéªŒè¦†ç›–ç‡
    """
    print("\n" + "="*70)
    print(">>> DATASET STATISTICS ANALYSIS <<<")
    print("="*70)
    
    # 1. æ€åŠ¿æå–æœ‰æ•ˆç‡ç»Ÿè®¡
    valid_gestalt_count = 0
    total_object_count = 0
    class_gestalt_stats = {cid: {'valid': 0, 'total': 0} for cid in range(2, 11)}
    
    print("\n[1/3] Analyzing Gestalt Extraction Coverage...")
    for i in range(min(len(dataset), 1000)):  # é‡‡æ ·å‰1000ä¸ªæ ·æœ¬
        sample = dataset[i]
        loss_mask = sample['loss_mask']
        gestalt_mask = sample['gestalt_mask']
        kg_class_ids = sample['kg_class_ids']
        
        for j in range(len(kg_class_ids)):
            if loss_mask[j] > 0:
                cid = kg_class_ids[j].item()
                if cid in class_gestalt_stats:
                    class_gestalt_stats[cid]['total'] += 1
                    if gestalt_mask[j] > 0:
                        class_gestalt_stats[cid]['valid'] += 1
                
                total_object_count += 1
                if gestalt_mask[j] > 0:
                    valid_gestalt_count += 1
    
    overall_coverage = 100 * valid_gestalt_count / max(total_object_count, 1)
    print(f"  Overall Gestalt Coverage: {valid_gestalt_count}/{total_object_count} ({overall_coverage:.1f}%)")
    
    print("\n  Per-Class Gestalt Coverage:")
    class_names = {2: "mountain", 3: "water", 4: "people", 5: "tree", 
                   6: "building", 7: "bridge", 8: "flower", 9: "bird", 10: "animal"}
    for cid in sorted(class_gestalt_stats.keys()):
        stats = class_gestalt_stats[cid]
        if stats['total'] > 0:
            ratio = 100 * stats['valid'] / stats['total']
            name = class_names.get(cid, f"cls_{cid}")
            print(f"    {name:12s} (ID {cid}): {stats['valid']:4d}/{stats['total']:4d} ({ratio:5.1f}%)")
    
    # 2. è¯­ä¹‰å…ˆéªŒè¦†ç›–ç‡
    print("\n[2/3] Analyzing Semantic Prior Coverage...")
    batch = dataset[0]
    coverage_stats = analyze_semantic_prior_coverage(
        batch['kg_class_ids'].unsqueeze(0), 
        batch['loss_mask'].unsqueeze(0)
    )
    print(f"  Semantic Prior Coverage: {coverage_stats['covered_objects']}/{coverage_stats['total_objects']} "
          f"({100*coverage_stats['coverage_rate']:.1f}%)")
    
    # 3. æ€åŠ¿æ•°å€¼åˆ†å¸ƒç»Ÿè®¡
    print("\n[3/3] Analyzing Gestalt Value Distributions...")
    gestalt_values = {'bias_x': [], 'bias_y': [], 'rotation': [], 'flow': []}
    
    for i in range(min(len(dataset), 500)):
        sample = dataset[i]
        target_boxes = sample['target_boxes']  # [N, 8]
        loss_mask = sample['loss_mask']        # [N]
        gestalt_mask = sample['gestalt_mask']  # [N]
        
        valid_mask = (loss_mask > 0) & (gestalt_mask > 0)
        if valid_mask.sum() > 0:
            valid_gestalt = target_boxes[valid_mask, 4:]  # [K, 4]
            gestalt_values['bias_x'].extend(valid_gestalt[:, 0].tolist())
            gestalt_values['bias_y'].extend(valid_gestalt[:, 1].tolist())
            gestalt_values['rotation'].extend(valid_gestalt[:, 2].tolist())
            gestalt_values['flow'].extend(valid_gestalt[:, 3].tolist())
    
    for key, values in gestalt_values.items():
        if len(values) > 0:
            mean_val = np.mean(values)
            std_val = np.std(values)
            min_val = np.min(values)
            max_val = np.max(values)
            print(f"  {key:12s}: mean={mean_val:6.3f}, std={std_val:6.3f}, range=[{min_val:6.3f}, {max_val:6.3f}]")
    
    print("="*70 + "\n")
    
    return {
        'gestalt_coverage': overall_coverage,
        'semantic_coverage': coverage_stats['coverage_rate'],
        'class_stats': class_gestalt_stats
    }


def main():
    # [NEW] æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="Train or RL-Finetune Poem2Layout")
    parser.add_argument('--rl_tuning', action='store_true', help="Enable Reinforcement Learning fine-tuning mode")
    parser.add_argument('--checkpoint', type=str, default=None, help="Path to pretrained model checkpoint (required for RL tuning)")
    parser.add_argument('--skip_analysis', action='store_true', help="Skip dataset statistics analysis")  # [NEW V10.0]
    parser.add_argument('--gestalt_strategy', type=str, default='progressive', 
                       choices=['fixed', 'progressive'], 
                       help="Gestalt loss weighting strategy")  # [NEW V10.0]
    args = parser.parse_args()

    # 1. Load config
    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "configs/default.yaml")
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    # 2. Init tokenizer and load FULL dataset for weight calculation
    model_config = config['model']
    train_config = config['training'] # è·å– training é…ç½®å—
    
    # é‡æ–°åˆå§‹åŒ–æ•°æ®é›†ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§
    dataset = PoegraphLayoutDataset(
        xlsx_path=model_config['xlsx_path'],
        labels_dir=model_config['labels_dir'],
        bert_model_path=model_config['bert_path'],
        max_layout_length=model_config['max_layout_length'],
        max_text_length=model_config['max_text_length']
    )
    
    # ==========================================================================
    # [NEW V10.0] æ•°æ®é›†è´¨é‡åˆ†æ
    # ==========================================================================
    if not args.skip_analysis:
        dataset_stats = analyze_dataset_statistics(dataset)
        
        # æ ¹æ®ç»Ÿè®¡ç»“æœç»™å‡ºå»ºè®®
        if dataset_stats['gestalt_coverage'] < 50.0:
            print("âš ï¸  [WARNING] Gestalt extraction coverage is low (<50%)!")
            print("    â†’ Consider adjusting thresholds in VisualGestaltExtractor")
            print("    â†’ Or increase semantic prior weight to compensate\n")
        
        if dataset_stats['semantic_coverage'] < 80.0:
            print("âš ï¸  [WARNING] Many objects are not covered by semantic priors!")
            print("    â†’ Consider adding more class IDs to SEMANTIC_GESTALT_PRIORS\n")
    
    # --- è®¡ç®—ç±»åˆ«æƒé‡ ---
    num_element_classes = model_config['num_classes'] # 9
    class_weights_tensor = compute_class_weights(dataset, num_element_classes)
    
    print(f"Calculated Class Weights (Internal 0:EOS, 1-9:Elements 2-10): {class_weights_tensor.tolist()}")
    # ---------------------------

    # 3. Init model (ä¼ å…¥æ‰€æœ‰æŸå¤±æƒé‡ï¼ŒåŒ…æ‹¬æ–°å¢çš„ Gestalt Loss Weight)
    print(f"\nInitializing model with latent_dim={model_config.get('latent_dim', 32)}...")
    model = Poem2LayoutGenerator(
        bert_path=model_config['bert_path'],
        num_classes=num_element_classes, # å®é™…å…ƒç´ ç±»åˆ«æ•° (9)
        # --- BBox Discrete Parameters (Legacy) ---
        num_bbox_bins=model_config.get('num_bbox_bins', 1000),
        bbox_embed_dim=model_config.get('bbox_embed_dim', 24),
        # --------------------------------------
        hidden_size=model_config['hidden_size'],
        bb_size=model_config['bb_size'],
        decoder_layers=model_config['decoder_layers'],
        decoder_heads=model_config['decoder_heads'],
        dropout=model_config['dropout'],
        
        # === CVAE å‚æ•° ===
        latent_dim=model_config.get('latent_dim', 32),
        
        # --- ä¼ å…¥æ‰€æœ‰æŸå¤±æƒé‡ ---
        coord_loss_weight=model_config.get('coord_loss_weight', 0.0),
        iou_loss_weight=model_config.get('iou_loss_weight', 1.0), 
        reg_loss_weight=model_config.get('reg_loss_weight', 1.0),    
        cls_loss_weight=model_config.get('cls_loss_weight', 0.0),    
        count_loss_weight=model_config.get('count_loss_weight', 0.0),
        area_loss_weight=model_config.get('area_loss_weight', 1.0),
        
        # æ ¸å¿ƒé€»è¾‘æƒé‡
        relation_loss_weight=model_config.get('relation_loss_weight', 5.0),
        overlap_loss_weight=model_config.get('overlap_loss_weight', 3.0),
        size_loss_weight=model_config.get('size_loss_weight', 2.0),
        
        # å®¡ç¾æƒé‡
        alignment_loss_weight=model_config.get('alignment_loss_weight', 0.0),
        balance_loss_weight=model_config.get('balance_loss_weight', 0.0),
        
        # [V5.4] èšç±»æŸå¤±æƒé‡
        clustering_loss_weight=model_config.get('clustering_loss_weight', 1.0),

        # [NEW V6.0] ä¸€è‡´æ€§æŸå¤±æƒé‡
        consistency_loss_weight=model_config.get('consistency_loss_weight', 1.0),
        
        # [NEW V10.0] è§†è§‰æ€åŠ¿æŸå¤±æƒé‡ (Visual Gestalt Supervision)
        # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œé»˜è®¤ 2.0
        gestalt_loss_weight=model_config.get('gestalt_loss_weight', 2.0),
        
        class_weights=class_weights_tensor 
        # -----------------------------------------
    )
    
    # ==========================================================================
    # [NEW V10.0] å°†è®­ç»ƒç­–ç•¥ä¼ é€’ç»™config
    # ==========================================================================
    config['gestalt_strategy'] = args.gestalt_strategy
    print(f"Gestalt Loss Strategy: {args.gestalt_strategy}")

    # 4. Split dataset and init data loaders
    total_size = len(dataset)
    train_size = int(0.8 * total_size) # 80%
    val_size = int(0.1 * total_size)   # 10%
    test_size = total_size - train_size - val_size # å‰©ä½™ä¸º 10%

    # æ‰§è¡Œ 80/10/10 éšæœºåˆ’åˆ†
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size, test_size]
    )
    print(f"\nDataset split: Train={train_size}, Validation={val_size}, Test={test_size}")

    # [NOTE] Batch Size è¯»å–è‡ªé…ç½®æ–‡ä»¶
    batch_size = train_config['batch_size']
    print(f"Using Batch Size: {batch_size}")

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=layout_collate_fn,
        num_workers=4, # å¤§æ‰¹é‡æ•°æ®å»ºè®®å¼€å¯å¤šçº¿ç¨‹åŠ è½½
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=layout_collate_fn,
        num_workers=4,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=layout_collate_fn,
        num_workers=4,
        pin_memory=True
    )
    
    # --- è·å– tokenizer å’Œä¸€ä¸ªå›ºå®šæ ·ä¾‹ ---
    tokenizer = dataset.tokenizer 
    # ä»éªŒè¯é›†ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·ä¾‹
    example_idx_in_full_dataset = val_dataset.indices[0]
    example_poem = dataset.data[example_idx_in_full_dataset]
    
    # **æ‰“å°å›ºå®šæ¨ç†æ ·ä¾‹çš„ KG å‘é‡å’Œç©ºé—´çŸ©é˜µ**
    print("\n" + "="*60)
    print(">>> INFERENCE EXAMPLE CONFIGURATION <<<")
    print("="*60)
    print(f"Poem: '{example_poem['poem']}'")
    print(f"GT Boxes: {example_poem['boxes']}")
    print("\n--- Knowledge Graph Debug ---")
    
    # 1. è§†è§‰å‘é‡
    kg_vector_example = dataset.pkg.extract_visual_feature_vector(example_poem['poem'])
    print(f"KG Vector (0:mountain(2), 1:water(3), ..., 8:animal(10)):")
    print(f"  {kg_vector_example.tolist()}")
    
    # 2. [NEW] ç©ºé—´çŸ©é˜µ
    kg_spatial_matrix_example = dataset.pkg.extract_spatial_matrix(example_poem['poem'])
    print("\nSpatial Matrix (9x9):")
    print(kg_spatial_matrix_example)
    print("="*60 + "\n")

    # 5. Logic Branch: RL Tuning OR Supervised Training
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    if args.rl_tuning:
        print("\n" + "="*70)
        print(">>> ENTERING RL FINE-TUNING MODE (SCST) <<<")
        print("="*70 + "\n")
        
        # 1. å¿…é¡»åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if args.checkpoint is None:
            raise ValueError("RL tuning requires a pretrained checkpoint! Use --checkpoint")
        
        print(f"Loading pretrained model from {args.checkpoint}...")
        # map_location ç¡®ä¿åœ¨ CPU/GPU é—´è¿ç§»å…¼å®¹
        checkpoint = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # 2. è¯»å– RL é…ç½®å‚æ•°
        rl_lr = float(train_config.get('rl_learning_rate', 5e-6))
        rl_epochs = int(train_config.get('rl_epochs', 50))
        
        print(f"RL Config -> Learning Rate: {rl_lr:.2e} (float) | Epochs: {rl_epochs} (int)")

        # 3. åˆå§‹åŒ– RLTrainer
        trainer = RLTrainer(model, train_loader, val_loader, config, tokenizer, example_poem, test_loader)
        
        # 4. å¼ºåˆ¶è¦†ç›–ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        for param_group in trainer.optimizer.param_groups:
            param_group['lr'] = rl_lr
        
        # [NEW] åˆå§‹åŒ–æœ€ä½³å¥–åŠ±è®°å½•
        best_reward = -float('inf')

        # 5. å¼€å§‹ RL è®­ç»ƒå¾ªç¯
        for epoch in range(rl_epochs):
            # [MODIFIED] æ¥æ”¶ train_rl_epoch è¿”å›çš„ avg_reward
            avg_reward = trainer.train_rl_epoch(epoch)
            
            # [NEW] å¯è§†åŒ–ï¼šæ¯ï¿½ï¿½ RL ç»“æŸç”Ÿæˆä¸€å¼ æ ·ä¾‹å›¾ï¼Œç›´è§‚çœ‹åˆ°æ¨¡å‹å˜åŒ–
            trainer._run_inference_example(epoch)
            
            # === [NEW] ä¿å­˜é€»è¾‘ A: ä¿å­˜æœ€æ£’çš„æ¨¡å‹ (Best Reward) ===
            # è¿™æ˜¯ infer.py ä¼˜å…ˆåŠ è½½çš„æ¨¡å‹
            if avg_reward > best_reward:
                best_reward = avg_reward
                best_save_path = os.path.join(train_config['output_dir'], "rl_best_reward.pth")
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'epoch': epoch,
                    'avg_reward': avg_reward,
                    'optimizer_state_dict': trainer.optimizer.state_dict(),
                    'rl_config': {'lr': rl_lr}
                }, best_save_path)
                print(f"ğŸŒŸ [New Best] Avg Reward {avg_reward:.4f} achieved! Model saved to {best_save_path}")

            # === [MODIFIED] ä¿å­˜é€»è¾‘ B: æ¯ 10 ä¸ª Epoch ä¿å­˜ä¸€æ¬¡ ===
            if (epoch + 1) % 100 == 0:
                rl_save_path = os.path.join(train_config['output_dir'], f"rl_finetuned_epoch_{epoch+1}.pth")
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'epoch': epoch,
                    'avg_reward': avg_reward,
                    'optimizer_state_dict': trainer.optimizer.state_dict(),
                    'rl_config': {'lr': rl_lr}
                }, rl_save_path)
                print(f"ğŸ’¾ [Checkpoint] Epoch {epoch+1} saved to {rl_save_path}")
                
    else:
        # åŸæœ‰çš„ç›‘ç£è®­ç»ƒé€»è¾‘
        print("\n" + "="*70)
        print(">>> STARTING SUPERVISED TRAINING (Text-to-Gestalt Enhanced) <<<")
        print("="*70)
        print(f"Total Epochs: {train_config['epochs']} | Batch Size: {batch_size}")
        print(f"Gestalt Loss Strategy: {args.gestalt_strategy}")
        print("="*70 + "\n")
        
        trainer = trainers.LayoutTrainer(model, train_loader, val_loader, config, tokenizer, example_poem, test_loader)
        trainer.train()

if __name__ == "__main__":
    main()